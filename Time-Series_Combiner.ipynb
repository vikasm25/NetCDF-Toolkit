{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93b2b05a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✔  Combined dataset saved to D:\\New folder\\Temp_series.nc\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Time-Series Combiner for NetCDF Files\n",
    "=====================================\n",
    "\n",
    "Steps performed\n",
    "---------------\n",
    "1. Gather all NetCDF files in INPUT_DIR that match FILE_GLOB.\n",
    "2. Extract a timestamp from each filename with REGEX_TIME.\n",
    "3. Open the files lazily (dask) and add the timestamp as a new\n",
    "   time coordinate.\n",
    "4. Concatenate along the \"time\" dimension.\n",
    "5. (Optional) Keep only the variables listed in VARS_TO_KEEP.\n",
    "6. Save the combined dataset to OUTPUT_FILE.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "from pathlib import Path\n",
    "import re\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "\n",
    "# ========= USER CONFIGURATION ==========================================\n",
    "INPUT_DIR      = r\"Path to Input Directory\"             # folder with NetCDF files\n",
    "FILE_GLOB      = \"Temp_*.nc\"                            # glob pattern relative to INPUT_DIR\n",
    "REGEX_TIME     = r\"(\\d{2})_(\\d{2})_(\\d{4})_(\\d{2})_(\\d{2})\"   # dd_mm_yyyy_hh_mm\n",
    "VARS_TO_KEEP   = None                                   # e.g. [\"Temp\"] or None for all\n",
    "OUTPUT_FILE    = r\"Path to Output Directory\"\n",
    "OVERWRITE      = False                                  # set True to replace existing file\n",
    "CHUNKS         = \"auto\"                                 # dask chunking for open_dataset\n",
    "# =======================================================================\n",
    "\n",
    "\n",
    "def extract_timestamp(path: Path, pattern: str) -> pd.Timestamp:\n",
    "    \"\"\"\n",
    "    Parse *pattern* from *path.name* and return a pandas.Timestamp.\n",
    "    Expected order in regex groups: DD MM YYYY HH MM.\n",
    "    \"\"\"\n",
    "    m = re.search(pattern, path.name)\n",
    "    if not m:\n",
    "        raise ValueError(f\"Cannot find timestamp in '{path.name}' \"\n",
    "                         f\"with pattern '{pattern}'\")\n",
    "    dd, mm, yyyy, hh, minute = m.groups()\n",
    "    return pd.Timestamp(f\"{yyyy}-{mm}-{dd} {hh}:{minute}\")\n",
    "\n",
    "\n",
    "def build_time_series(in_dir: Path,\n",
    "                      file_glob: str,\n",
    "                      regex_time: str,\n",
    "                      vars_to_keep=None,\n",
    "                      chunks=\"auto\"):\n",
    "    \"\"\"Return an xarray Dataset concatenated along a new 'time' dimension.\"\"\"\n",
    "    files = sorted(in_dir.glob(file_glob))\n",
    "    if not files:\n",
    "        raise FileNotFoundError(f\"No files match '{file_glob}' in {in_dir}\")\n",
    "\n",
    "    datasets = []\n",
    "    for fp in files:\n",
    "        ts = extract_timestamp(fp, regex_time)\n",
    "        ds = xr.open_dataset(fp, chunks=chunks)\n",
    "        if vars_to_keep is not None:\n",
    "            missing = [v for v in vars_to_keep if v not in ds]\n",
    "            if missing:\n",
    "                raise KeyError(f\"{fp.name}: missing var(s) {missing}\")\n",
    "            ds = ds[vars_to_keep]\n",
    "        ds = ds.expand_dims(time=[ts])         # add new coordinate\n",
    "        datasets.append(ds)\n",
    "\n",
    "    combined = xr.concat(datasets, dim=\"time\")\n",
    "    return combined.sortby(\"time\")            # ensure chronological order\n",
    "\n",
    "\n",
    "def main():\n",
    "    in_dir  = Path(INPUT_DIR).expanduser().resolve()\n",
    "    out_fp  = Path(OUTPUT_FILE).expanduser().resolve()\n",
    "    if out_fp.exists() and not OVERWRITE:\n",
    "        raise FileExistsError(f\"{out_fp} exists (set OVERWRITE=True).\")\n",
    "\n",
    "    combined_ts = build_time_series(in_dir,\n",
    "                                    FILE_GLOB,\n",
    "                                    REGEX_TIME,\n",
    "                                    vars_to_keep=VARS_TO_KEEP,\n",
    "                                    chunks=CHUNKS)\n",
    "\n",
    "    combined_ts.to_netcdf(out_fp, mode=\"w\", engine=\"netcdf4\")\n",
    "    print(f\"✔  Combined dataset saved to {out_fp}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8fb5d7c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "WIN_PY",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
